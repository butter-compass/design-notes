* What is Butter Compass?
  
  *Note: none of this is set in stone - it's all open for discussion*

  Butter Compass (other than being a tool to find butter) provides a
  system for running and managing functions that respond to events,
  built on top of OpenShift Enterprise v3.

  The events are persistent, so functions can be deployed to process
  one or more streams starting with the most recent and all future
  messages, or at any offset in the stream.

  Functions can be written in any supported language, which would
  initially include Java (and other JVM langs), JS, possibly Python or
  Ruby, possibly Rust or Swift.

  Butter Compass is based on a synthesis of the goals of lambda
  systems, EIP systems, and event processing, and is somewhat between
  SaaS and PaaS (can be thought of as a higher-level PaaS, perhaps)
  (see the [[*Prior%20Art][Prior Art]] section below).

** What isn't Butter Compass?

   - Not a per-cycle cost-saving tool
   - Not a strict resource limiting tool
   - Not a general web/API platform

* System structure

  Note: This is purely conceptual - all of the components would likely
  be running in the same container mananger (kube), and the deployer,
  timer service, web ui, etc may be functions themselves,
  communicating with other components via the broker. This would also
  vet the implementation as it is being built, since these would be
  non-trivial functions.

  [[./imgs/butter-compass-overview.png]]

** Event Broker
   
   Stores/delivers events

   built with: Kafka ?

   
** Event Bridge

   maps external events within the enterprise onto the broker (and
   viceversa)
   
   built with: Camel ?

** Timer Service

   emits timer-based events on the requested schedules to the
   requested streams

   wrapper around Quartz (immutant scheduling), Camel timer?

** fn-container (fnbox?)

   wraps a function, mapping events into the native event format, and
   provides an environment/context for the fn to communicate out with

   one impl. per platform/language, as a docker container

** Load Monitor

   watches the load on the streams and the fn-containers to
   determine when new fn-containers need to be launched, or extra
   ones need to be killed

   built from scratch, feeding data to kube

** Container Manager

   Manages creating/destroying/monitoring fn-containers

   kube + some more efficient way to handle multiple function
   instances (see [[*Challenges][Challenges]] below)

** Web Gateway
   converts web requests into request-response  events to be
   handled by a fn. Would also handle web auth.

   APIMan, with plugin to convert requests into request-response
   events. Or can camel do this as well? (I guess not APIMan now, ffs)

** Deployer

   handles setting up the build for the fn-container, and
   wires up the streams and gateway route (if the fn is a web fn)
   based on the deployment it is given

   scratch-made biscuit

** Tracer

   Traces events from entry to exit/completion, providing
   timing data

** Logger

   Collects logging info from functions

** Web UI

   Provides a UI into trace, log, and deployment.

* Deployment structure

  Jar or zip containing the function and the function's
  dependencies. The archive could contain multiple functions - each
  one would need to be provided a descriptor to wire it up.

** Descriptor

   Defines how the fn should be deployed/wired/limited. Is provided
   along with the deploy, or provided after the fact to set/alter the
   wiring.

   The example is yaml, but that decision (or any decision, really)
   hasn't been fixed (hand-writing json sure is godawful though).

#+BEGIN_SRC yaml
  # a single descriptor can wire multiple fns
  platform: node # or java, etc
  name: foo
  description: yada yada # optional?
  - fn: org.blah.Blah # a platform-specific locator for the function
    bindings: # one of route, timer, or streams is required
      # route from web request to this fn
      route:
        context: /foo
        stream: foo # auto bind to this stream
      # or call on a schedule
      timer: <cron-spec>
      # or attach to these streams
      streams:
        - name: address-1
          offset: 42 # optional, one of first, latest, or number. defaults to latest
        - name: address-2 # can bind to multple streams
      # write the return value of the fn to a stream - optional
      out: address-3
    limits:
      # resource limits here - TBD
#+END_SRC


* fn API

** Event structure

   An event will consist of metadata and the event data, translated
   into an appropriate data structure for the platform. 
   
   Metadata may include:
   
   - event id
   - source stream id
   - stream offset (stream id + offset may be enough to replace event id)
   - tracing identifiers
   - response stream id (if request-response)
   - ttl + start time (after which the event is no longer valid, and,
     if a request-response message, the client has given up).
   - http headers (if a web request)

  The function API will differ depending on the platform to provide an
  idiomatic interface. 

  Each function will be given the event and a context that allows
  interaction with the system environment (generating other events,
  querying for the remaining ttl, logging).

  Possible js API:

#+BEGIN_SRC javascript
  exports.foo = function(event, context, callback) {
      context.log("Handling event: " + event.id);
      
      if (event.data.somevalue) {

          // emit an event and expect a response event
          // .emit will set appropriate metadata, including parent tracing
          // ids from event, and updated ttl(?)
          // TODO: needs some way to set additional metadata?
          context.emit("somestream", {foo: "bar"}, function(err, ev) {
              // signal completion using the callback
              callback(err, ev.data)
          });

          // fire and forget
          context.emit("anotherstream", someData);
          
      } else {
          callback(null, some_other_reply)
      }
  }

#+END_SRC

  Possible, crappy, java API:

#+BEGIN_SRC java
  public class Whatever {
      // return anything jacksonable?
      public static String handleSomething(Event event, Context ctx) {
          ctx.log().info("Handing event: " + event.id);

          final Event downstreamEvent = ctx.createEvent();
          downstreamEvent.put("foo", "bar");

          // FIXME: this is sync, and far from correct
          if (event.data().getBoolean("somevalue", false)) {
              // fire and forget
              ctx.emit("anotherstream", someData);
              
              return ctx.emitSync("somestream", downstreamEvent).data();
          } else {
              return aDefaultResponse;
          }
      }
  }

#+END_SRC

* State

** fn State
   
   Functions can't rely on any local state (disk, memory). Any storage
   has to be in an external system.

** System State
   
   Each component in BC should be as stateless as possible,
   pulling/storing all state elsewhere (etcd?). The Event broker
   itself will need reliable storage for the event streams.

* Challenges

  - figuring out a way to handle fn's in a resource efficient
    manner. For JVM-based fns, if every fn gets a JVM, that can eat a
    lot of memory, and that's just one resource concern.
  - some non-AWS lambda systems assume a docker container per function
    instance, which is terribly wasteful - for this to succeed, it
    needs to have a unit of deployment that is more granular
  - per-address authorization
  - giving users adequate testing tools w/o requiring a running system
  - making the fn-container interface simple enough to make an
    implementation for a new platform straightforward
  - if Kafka is used as the broker, we would have to deal with the
    challenges of running a stable Kafka cluster on OpenShift, but
    others are working on that
  - bindings/deployment for each language should meet the language on
    its terms - don't force maven or any other alien tool

* Potential uses

  - ??

* Prior Art

  Draws inspiration from:

  - [[https://aws.amazon.com/lambda/][AWS Lambda]]
  - [[https://azure.microsoft.com/en-us/services/functions/][Azure Functions]]
  - [[https://new-console.ng.bluemix.net/openwhisk/][IBM BlueMix OpenWhisk]]
  - [[https://servicemix.apache.org/][Apache ServiceMix]]
  - [[http://debezium.io/][Debezium]] (its original incarnation)

* Random notes

  - The Web Gateway is for triggering events, it's not designed for
    full-blown web applications/APIs (unless the payloads the API
    traffics is fairly small).
  - Event Streams are broadcast - every fn attached to the stream will
    get the message.  Note that only /one/ instance of the fn will
    receive the message
  - Metrics - what do we need beyond tracing?
  - =context= needs circuit-breaker support for =emit= calls.  This
    means a central location to store the state of those breakers
  - How are fns tested locally, since they require a =context=?
  - If the broker supported STOMP, that would make building platform
    impls simpler
  - build for failure - don't ever assume a clean shutdown of any
    component
  - what about authn/authz? Keycloak at the gateway, but how to
    authorize what events a fn can see?
  - given the setup difficultly (requiring OSE, many moving parts),
    this would probably be straight to product if built
  - what about back-pressure?
